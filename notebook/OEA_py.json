{
	"name": "OEA_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false
		},
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"from delta.tables import DeltaTable\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.utils import AnalysisException\n",
					"import pandas as pd\n",
					"import sys\n",
					"import re\n",
					"import json\n",
					"import datetime\n",
					"import pytz\n",
					"import random\n",
					"import io\n",
					"import logging\n",
					"\n",
					"logger = logging.getLogger('OEA')\n",
					"\n",
					"class OEA:\n",
					"    def __init__(self, storage_account='', instrumentation_key=None, salt='', logging_level=logging.DEBUG):\n",
					"        if storage_account:\n",
					"            self.storage_account = storage_account\n",
					"        else:\n",
					"            oea_id = mssparkutils.env.getWorkspaceName()[8:] # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\n",
					"            self.storage_account = 'stoea' + oea_id # sets the name of the storage account based on OEA naming convention\n",
					"            self.keyvault = 'kv-oea-' + oea_id\n",
					"        self.keyvault_linked_service = 'LS_KeyVault_OEA'\n",
					"        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\n",
					"        self._initialize_logger(instrumentation_key, logging_level)\n",
					"        self.salt = salt\n",
					"        self.timezone = 'EST'\n",
					"        self.stage1np = 'abfss://stage1np@' + self.storage_account + '.dfs.core.windows.net'\n",
					"        self.stage2np = 'abfss://stage2np@' + self.storage_account + '.dfs.core.windows.net'\n",
					"        self.stage2p = 'abfss://stage2p@' + self.storage_account + '.dfs.core.windows.net'\n",
					"        self.stage3np = 'abfss://stage3np@' + self.storage_account + '.dfs.core.windows.net'\n",
					"        self.stage3p = 'abfss://stage3p@' + self.storage_account + '.dfs.core.windows.net'\n",
					"        self.framework_path = 'abfss://oea-framework@' + self.storage_account + '.dfs.core.windows.net'\n",
					"\n",
					"        # Initialize framework db\n",
					"        spark.sql(f\"CREATE DATABASE IF NOT EXISTS oea\")\n",
					"        spark.sql(f\"CREATE TABLE IF NOT EXISTS oea.env (name string not null, value string not null, description string) USING DELTA LOCATION '{self.framework_path}/db/env'\")\n",
					"        df = spark.sql(\"select value from oea.env where name='storage_account'\")\n",
					"        if df.first(): spark.sql(f\"UPDATE oea.env set value='{self.storage_account}' where name='storage_account'\")\n",
					"        else: spark.sql(f\"INSERT INTO oea.env VALUES ('storage_account', '{self.storage_account}', 'The name of the data lake storage account for this OEA instance.')\")\n",
					"        spark.sql(f\"CREATE TABLE IF NOT EXISTS OEA.watermark (source string not null, entity string not null, watermark timestamp not null) USING DELTA LOCATION '{self.framework_path}/db/watermark'\")\n",
					"\n",
					"        logger.debug(\"OEA initialized.\")\n",
					"    \n",
					"    def path(self, container_name, directory_path=None):\n",
					"        if directory_path:\n",
					"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net/{directory_path}'\n",
					"        else:\n",
					"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net'\n",
					"\n",
					"    def convert_path(self, path):\n",
					"        \"\"\" Converts the given path into a valid url.\n",
					"            eg, convert_path('stage1np/contoso_sis/student/*') # returns abfss://stage1np@storageaccount.dfs.core.windows.net/contoso_sis/student/*\n",
					"        \"\"\"\n",
					"        path_args = path.split('/')\n",
					"        stage = path_args.pop(0)\n",
					"        return self.path(stage, '/'.join(path_args))            \n",
					"\n",
					"    def _initialize_logger(self, instrumentation_key, logging_level):\n",
					"        logging.lastResort = None\n",
					"        # the logger will print an error like \"ValueError: I/O operation on closed file\" because we're trying to have log messages also print to stdout\n",
					"        # and apparently this causes issues on some of the spark executor nodes. The bottom line is that we don't want these logging errors to get printed in the notebook output.\n",
					"        logging.raiseExceptions = False\n",
					"        logger.setLevel(logging_level)\n",
					"\n",
					"        handler = logging.StreamHandler(sys.stdout)\n",
					"        handler.setLevel(logging_level)\n",
					"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
					"        handler.setFormatter(formatter)\n",
					"        logger.addHandler(handler)\n",
					"\n",
					"    def get_value_from_db(self, query):\n",
					"        df = spark.sql(query)\n",
					"        if df.first(): return df.first()[0]\n",
					"        else: return None\n",
					"\n",
					"    def get_last_watermark(self, source, entity):\n",
					"        return self.get_value_from_db(f\"select w.watermark from oea.watermark w where w.source='{source}' and w.entity='{entity}' order by w.watermark desc\")\n",
					"\n",
					"    def insert_watermark(self, source, entity, watermark_datetime):\n",
					"        spark.sql(f\"insert into oea.watermark values ('{source}', '{entity}', '{watermark_datetime}')\")\n",
					"\n",
					"    def get_secret(self, secret_name):\n",
					"        \"\"\" Retrieves the specified secret from the keyvault.\n",
					"            This method assumes that the keyvault linked service has been setup and is accessible.\n",
					"        \"\"\"\n",
					"        sc = SparkSession.builder.getOrCreate()\n",
					"        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
					"        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \n",
					"        return value\n",
					"\n",
					"    def delete(self, path):\n",
					"        oea.rm_if_exists(self.convert_path(path))\n",
					"\n",
					"    def land(self, data_source, entity, df, partition_label='', format_str='csv', header=True, mode='overwrite'):\n",
					"        \"\"\" Lands data in stage1np. If partition label is not provided, the current datetime is used with the label of 'batchdate'.\n",
					"            eg, land('contoso_isd', 'student', data, 'school_year=2021')\n",
					"        \"\"\"\n",
					"        tz = pytz.timezone(self.timezone)\n",
					"        datetime_str = datetime.datetime.now(tz).replace(microsecond=0).isoformat()\n",
					"        datetime_str = datetime_str.replace(':', '') # Path names can't have a colon - https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md#path-names\n",
					"        df.write.format(format_str).save(self.path('stage1np', f'{data_source}/{entity}/{partition_label}/batchdate={datetime_str}'), header=header, mode=mode)\n",
					"\n",
					"    def load(self, folder, table, stage=None, data_format='delta'):\n",
					"        \"\"\" Loads a dataframe based on the path specified in the given args \"\"\"\n",
					"        if stage is None: stage = self.stage2p\n",
					"        path = f\"{stage}/{folder}/{table}\"\n",
					"        try:\n",
					"            df = spark.read.load(f\"{stage}/{folder}/{table}\", format=data_format)\n",
					"            return df        \n",
					"        except AnalysisException as e:\n",
					"            raise ValueError(\"Failed to load. Are you sure you have the right path?\\nMore info below:\\n\" + str(e)) \n",
					"\n",
					"    def load_csv(self, path, header=True):\n",
					"        \"\"\" Loads a dataframe based on the path specified \n",
					"            eg, df = load_csv('stage1np/example/student/*')\n",
					"        \"\"\"\n",
					"        url_path = self.convert_path(path)\n",
					"        try:\n",
					"            df = spark.read.load(url_path, format='csv', header=header)\n",
					"            return df        \n",
					"        except AnalysisException as e:\n",
					"            raise ValueError(f\"Failed to load from: {url_path}. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\n",
					"\n",
					"    def load_delta(self, path):\n",
					"        \"\"\" Loads a dataframe based on the path specified \n",
					"            eg, df = load_delta('stage2np/example/student/*')\n",
					"        \"\"\"\n",
					"        url_path = self.convert_path(path)\n",
					"        try:\n",
					"            df = spark.read.load(url_path, format='delta')\n",
					"            return df        \n",
					"        except AnalysisException as e:\n",
					"            raise ValueError(f\"Failed to load from: {url_path}. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\n",
					"\n",
					"    def load_from_stage1(self, path_and_filename, data_format='csv', header=True):\n",
					"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\n",
					"        path = f\"{self.stage1np}/{path_and_filename}\"\n",
					"        df = spark.read.load(path, format=data_format, header=header)\n",
					"        return df        \n",
					"\n",
					"    def load_sample_from_csv_file(self, path_and_filename, header=True, stage=None):\n",
					"        \"\"\" Loads a sample from the specified csv file and returns a pandas dataframe.\n",
					"            Ex: print(load_sample_from_csv_file('/student_data/students.csv'))\n",
					"        \"\"\"\n",
					"        if stage is None: stage = self.stage1np\n",
					"        csv_str = mssparkutils.fs.head(f\"{stage}/{path_and_filename}\") # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#preview-file-content\n",
					"        complete_lines = re.match(r\".*\\n\", csv_str, re.DOTALL).group(0)\n",
					"        if header: header = 0 # for info on why this is needed: https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\n",
					"        else: header = None\n",
					"        pdf = pd.read_csv(io.StringIO(complete_lines), sep=',', header=header)\n",
					"        return pdf\n",
					"\n",
					"    def print_stage(self, path):\n",
					"        \"\"\" Prints out the highlevel contents of the specified stage.\"\"\"\n",
					"        msg = path + \"\\n\"\n",
					"        folders = self.get_folders(path)\n",
					"        for folder_name in folders:\n",
					"            entities = self.get_folders(path + '/' + folder_name)\n",
					"            msg += f\"{folder_name}: {entities}\\n\"\n",
					"        print(msg)            \n",
					"\n",
					"    def fix_column_names(self, df):\n",
					"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
					"        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\n",
					"        return df_with_valid_column_names\n",
					"\n",
					"    def to_spark_schema(self, schema):#: list[list[str]]):\n",
					"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
					"            Example:\n",
					"            schemas['Person'] = [['Id','string','hash'],\n",
					"                                    ['CreateDate','timestamp','no-op'],\n",
					"                                    ['LastModifiedDate','timestamp','no-op']]\n",
					"            to_spark_schema(schemas['Person'])\n",
					"        \"\"\"\n",
					"        fields = []\n",
					"        for col_name, dtype, op in schema:\n",
					"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
					"        spark_schema = StructType(fields)\n",
					"        return spark_schema\n",
					"\n",
					"    def ingest_incremental_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\n",
					"        \"\"\" Processes incremental batch data from stage1 into stage2 \"\"\"\n",
					"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
					"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
					"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
					"        logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\n",
					"\n",
					"        if has_header: header_flag = 'true'\n",
					"        else: header_flag = 'false'\n",
					"        spark_schema = self.to_spark_schema(schema)\n",
					"        df = spark.readStream.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
					"        #df = spark.read.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
					"        #display(df)\n",
					"        #df = df.withColumn('batchdate', F.to_timestamp(df.batchdate, \"yyyy-MM-dd'T'HHmmssZ\"))\n",
					"        df = df.dropDuplicates([primary_key]) # drop duplicates across batches. More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\n",
					"        \n",
					"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
					"\n",
					"        if len(df_pseudo.columns) == 0:\n",
					"            logger.info('No data to be written to stage2p')\n",
					"        else:        \n",
					"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints/incremental_p').partitionBy(partition_by)\n",
					"            query = query.start(p_destination_path)\n",
					"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
					"            logger.info(query.lastProgress)\n",
					"\n",
					"        if len(df_lookup.columns) == 0:\n",
					"            logger.info('No data to be written to stage2np')\n",
					"        else:\n",
					"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints/incremental_np').partitionBy(partition_by)\n",
					"            query2 = query2.start(np_destination_path)\n",
					"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
					"            logger.info(query2.lastProgress)        \n",
					"\n",
					"    def _merge_into_table(self, df, destination_path, checkpoints_path, condition):\n",
					"        \"\"\" Merges data from the given dataframe into the delta table at the specified destination_path, based on the given condition.\n",
					"            If not delta table exists at the specified destination_path, a new delta table is created and the data from the given dataframe is inserted.\n",
					"            eg, merge_into_table(df_lookup, np_destination_path, source_path + '/_checkpoints/delta_np', \"current.id_pseudonym = updates.id_pseudonym\")\n",
					"        \"\"\"\n",
					"        if DeltaTable.isDeltaTable(spark, destination_path):      \n",
					"            dt = DeltaTable.forPath(spark, destination_path)\n",
					"            def upsert(batch_df, batchId):\n",
					"                dt.alias(\"current\").merge(batch_df.alias(\"updates\"), condition).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()                \n",
					"            query = df.writeStream.format(\"delta\").foreachBatch(upsert).outputMode(\"update\").trigger(once=True).option(\"checkpointLocation\", checkpoints_path)\n",
					"        else:\n",
					"            logger.info(f'Delta table does not yet exist at {destination_path} - creating one now and inserting initial data.')\n",
					"            query = df.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", checkpoints_path)\n",
					"        query = query.start(destination_path)\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
					"        logger.info(query.lastProgress)    \n",
					"\n",
					"    def ingest_delta_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\n",
					"        \"\"\" Processes delta batch data from stage1 into stage2 \"\"\"\n",
					"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
					"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
					"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
					"        logger.info(f'Processing delta data from: {source_path} and writing out to: {p_destination_path}')\n",
					"\n",
					"        if has_header: header_flag = 'true'\n",
					"        else: header_flag = 'false'\n",
					"        spark_schema = self.to_spark_schema(schema)\n",
					"        df = spark.readStream.load(source_path + '/*', format=data_format, header=header_flag, schema=spark_schema)\n",
					"        \n",
					"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
					"\n",
					"        if len(df_pseudo.columns) == 0:\n",
					"            logger.info('No data to be written to stage2p')\n",
					"        else:\n",
					"            self._merge_into_table(df_pseudo, p_destination_path, source_path + '/_checkpoints/delta_p', \"current.id_pseudonym = updates.id_pseudonym\")\n",
					"\n",
					"        if len(df_lookup.columns) == 0:\n",
					"            logger.info('No data to be written to stage2np')\n",
					"        else:\n",
					"            self._merge_into_table(df_lookup, np_destination_path, source_path + '/_checkpoints/delta_np', \"current.id_pseudonym = updates.id_pseudonym\")\n",
					"\n",
					"    def ingest_snapshot_data(self, source_system, tablename, schema, partition_by, primary_key='id', data_format='csv', has_header=True):\n",
					"        \"\"\" Processes snapshot batch data from stage1 into stage2 \"\"\"\n",
					"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
					"        latest_batch = self.get_latest_folder(source_path)\n",
					"        source_path = source_path + '/' + latest_batch\n",
					"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
					"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
					"        logger.info(f'Processing snapshot data from: {source_path} and writing out to: {p_destination_path}')\n",
					"\n",
					"        if has_header: header_flag = 'true'\n",
					"        else: header_flag = 'false'\n",
					"        spark_schema = self.to_spark_schema(schema)\n",
					"        df = spark.read.load(source_path, format=data_format, header=header_flag, schema=spark_schema)\n",
					"        df = df.dropDuplicates([primary_key]) # More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication\n",
					"        \n",
					"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
					"\n",
					"        if len(df_pseudo.columns) == 0:\n",
					"            logger.info('No data to be written to stage2p')\n",
					"        else:\n",
					"            df_pseudo.write.save(p_destination_path, format='delta', mode='overwrite', partitionBy=partition_by) \n",
					"\n",
					"        if len(df_lookup.columns) == 0:\n",
					"            logger.info('No data to be written to stage2np')\n",
					"        else:\n",
					"            df_lookup.write.save(np_destination_path, format='delta', mode='overwrite', partitionBy=partition_by) \n",
					"\n",
					"    def pseudonymize(self, df, schema): #: list[list[str]]):\n",
					"        \"\"\" Performs pseudonymization of the given dataframe based on the provided schema.\n",
					"            For example, if the given df is for an entity called person, \n",
					"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
					"            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
					"            and the non-masked values for columns marked to be masked.\"\"\"\n",
					"        \n",
					"        df_pseudo = df_lookup = df\n",
					"\n",
					"        for col_name, dtype, op in schema:\n",
					"            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
					"                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
					"                df_lookup = df_lookup.drop(col_name)           \n",
					"            elif op == \"hash\" or op == 'h':\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
					"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\n",
					"            elif op == \"mask\" or op == 'm':\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
					"            elif op == \"partition-by\":\n",
					"                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
					"            elif op == \"no-op\" or op == 'x':\n",
					"                df_lookup = df_lookup.drop(col_name)\n",
					"\n",
					"        df_pseudo = self.fix_column_names(df_pseudo)\n",
					"        df_lookup = self.fix_column_names(df_lookup)\n",
					"\n",
					"        return (df_pseudo, df_lookup)\n",
					"\n",
					"    # Returns true if the path exists\n",
					"    def path_exists(self, path):\n",
					"        tableExists = False\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(path)\n",
					"            tableExists = True\n",
					"        except Exception as e:\n",
					"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
					"            pass\n",
					"        return tableExists\n",
					"\n",
					"    def ls(self, path):\n",
					"        if not path.startswith(\"abfss:\"):\n",
					"            path = self.convert_path(path)\n",
					"        folders = []\n",
					"        files = []\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(path)\n",
					"            for item in items:\n",
					"                if item.isFile:\n",
					"                    files.append(item.name)\n",
					"                elif item.isDir:\n",
					"                    folders.append(item.name)\n",
					"        except Exception as e:\n",
					"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
					"        return (folders, files)\n",
					"\n",
					"    def print_stage(self, path):\n",
					"        print(path)\n",
					"        folders = self.get_folders(path)\n",
					"        for folder_name in folders:\n",
					"            entities = self.get_folders(path + '/' + folder_name)\n",
					"            print(f\"{folder_name}: {entities}\")\n",
					"\n",
					"    # Return the list of folders found in the given path.\n",
					"    def get_folders(self, path):\n",
					"        dirs = []\n",
					"        try:\n",
					"            items = mssparkutils.fs.ls(path)\n",
					"            for item in items:\n",
					"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
					"                if item.isDir:\n",
					"                    dirs.append(item.name)\n",
					"        except Exception as e:\n",
					"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
					"        return dirs\n",
					"\n",
					"    def get_latest_folder(self, path):\n",
					"        folders = self.get_folders(path)\n",
					"        if len(folders) > 0: return folders[-1]\n",
					"        else: return None\n",
					"\n",
					"    # Remove a folder if it exists (defaults to use of recursive removal).\n",
					"    def rm_if_exists(self, path, recursive_remove=True):\n",
					"        try:\n",
					"            mssparkutils.fs.rm(path, recursive_remove)\n",
					"        except Exception as e:\n",
					"            pass\n",
					"\n",
					"    def pop_from_path(self, path):\n",
					"        \"\"\" Pops the last arg in a path and returns the path and the last arg as a tuple.\n",
					"            pop_from_path('abfss://stage2@xyz.dfs.core.windows.net/ms_insights/test.csv') # returns ('abfss://stage2@xyz.dfs.core.windows.net/ms_insights', 'test.csv')\n",
					"        \"\"\"\n",
					"        m = re.match(r\"(.*)\\/([^/]+)\", path)\n",
					"        return (m.group(1), m.group(2))\n",
					"\n",
					"    def parse_source_path(self, path):\n",
					"        \"\"\" Parses a path that looks like this: abfss://stage2p@stoeacisd3ggimpl3.dfs.core.windows.net/ms_insights\n",
					"            and returns a dictionary like this: {'stage_num': '2', 'ss': 'ms_insights'}\n",
					"            Note that it will also return a 'stage_num' of 2 if the path is stage2p or stage2np - this is by design because the spark db with the s2 prefix will be used for data in stage2 and stage2p.\n",
					"        \"\"\"\n",
					"        m = re.match(r\".*:\\/\\/stage(?P<stage_num>\\d+)[n]?[p]?@[^/]+\\/(?P<ss>[^/]+)\", path)\n",
					"        return m.groupdict()\n",
					"    \n",
					"    def create_lake_db(self, stage_num, source_dir, source_format='DELTA'):\n",
					"        \"\"\" Creates a spark db that points to data in the given stage under the specified source directory (assumes that every folder in the source_dir is a table).\n",
					"            Example: create_lake_db(2, 'contoso_sis')\n",
					"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
					"        \"\"\"\n",
					"        db_name = f's{stage_num}_{source_dir}'\n",
					"        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
					"        self.create_lake_views(db_name, self.path(f'stage{stage_num}p', source_dir), source_format)\n",
					"        self.create_lake_views(db_name, self.path(f'stage{stage_num}np', source_dir), source_format)\n",
					"        result = \"Database created: \" + db_name\n",
					"        logger.info(result)\n",
					"        return result        \n",
					"\n",
					"    def create_lake_views(self, db_name, source_path, source_format):\n",
					"        dirs = self.get_folders(source_path)\n",
					"        for table_name in dirs:\n",
					"            spark.sql(f\"create table if not exists {db_name}.{table_name} using {source_format} location '{source_path}/{table_name}'\")\n",
					"\n",
					"    def drop_lake_db(self, db_name):\n",
					"        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
					"        result = \"Database dropped: \" + db_name\n",
					"        logger.info(result)\n",
					"        return result       \n",
					"\n",
					"    def create_sql_db(self, stage_num, source_dir, source_format='DELTA'):\n",
					"        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\n",
					"        db_name = f'sqls{stage_num}_{source_dir}'\n",
					"        cmd += '-- Create a new sql script then execute the following in it:'\n",
					"        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\n",
					"        cmd += f\"USE {db_name};\\nGO\\n\\n\"\n",
					"        cmd += self.create_sql_views(self.path(f'stage{stage_num}p', source_dir), source_format)\n",
					"        cmd += self.create_sql_views(self.path(f'stage{stage_num}np', source_dir), source_format)\n",
					"        print(cmd)\n",
					"\n",
					"    def create_sql_views(self, source_path, source_format):\n",
					"        cmd = ''      \n",
					"        dirs = self.get_folders(source_path)\n",
					"        for table_name in dirs:\n",
					"            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{source_path}/{table_name}', FORMAT='{source_format}') AS [r];\\nGO\\n\"\n",
					"        return cmd\n",
					"\n",
					"    def drop_sql_db(self, db_name):\n",
					"        print('Click on the menu next to the SQL db and select \"Delete\"')\n",
					"\n",
					"    # List installed packages\n",
					"    def list_packages(self):\n",
					"        import pkg_resources\n",
					"        for d in pkg_resources.working_set:\n",
					"            print(d)\n",
					"\n",
					"    def print_schema_starter(self, entity_name, df):\n",
					"        \"\"\" Prints a starter schema that can be modified as needed when developing the oea schema for a new module. \"\"\"\n",
					"        st = f\"self.schemas['{entity_name}'] = [\"\n",
					"        for col in df.schema:\n",
					"            st += f\"['{col.name}', '{str(col.dataType)[:-4].lower()}', 'no-op'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\n",
					"        return st[:-11] + ']'\n",
					"\n",
					"    def write_rows_as_csv(data, folder, filename, container=None):\n",
					"        \"\"\" Writes a dictionary as a csv to the specified location. This is helpful when creating test data sets and landing them in stage1np.\n",
					"            data = [{'id':'1','fname':'John'}, {'id':'1','fname':'Jane'}]\n",
					"        \"\"\"\n",
					"        if container == None: container = self.stage1np\n",
					"        pdf = pd.DataFrame(data)\n",
					"        mssparkutils.fs.put(f\"{container}/{folder}/{filename}\", pdf.to_csv(index=False), True) # True indicates overwrite mode  \n",
					"\n",
					"    def write_rowset_as_csv(data, folder, container=None):\n",
					"        \"\"\" Writes out as csv rows the passed in data. The inbound data should be in a format like this:\n",
					"            data = { 'students':[{'id':'1','fname':'John'}], 'courses':[{'id':'31', 'name':'Math'}] }\n",
					"        \"\"\"\n",
					"        if container == None: container = self.stage1np\n",
					"        for entity_name, value in data.items():\n",
					"            pdf = pd.DataFrame(value)\n",
					"            mssparkutils.fs.put(f\"{container}/{folder}/{entity_name}.csv\", pdf.to_csv(index=False), True) # True indicates overwrite mode         \n",
					"\n",
					"    def create_empty_dataframe(self, schema):\n",
					"        \"\"\" Creates an empty dataframe based on the given schema which is specified as an array of column names and sql types.\n",
					"            eg, schema = [['data_source','string'], ['entity','string'], ['watermark','timestamp']]\n",
					"        \"\"\"\n",
					"        fields = []\n",
					"        for col_name, col_type in schema:\n",
					"            fields.append(StructField(col_name, globals()[col_type.lower().capitalize() + \"Type\"](), True))\n",
					"        spark_schema = StructType(fields)\n",
					"        df = spark.createDataFrame(spark.sparkContext.emptyRDD(), spark_schema)\n",
					"        return df\n",
					"\n",
					"    def delete_data_source(self, data_source):\n",
					"        self.rm_if_exists(self.convert_path(f'stage1np/{data_source}'))\n",
					"        self.rm_if_exists(self.convert_path(f'stage2np/{data_source}'))\n",
					"        self.rm_if_exists(self.convert_path(f'stage2p/{data_source}'))\n",
					"\n",
					"class BaseOEAModule:\n",
					"    \"\"\" Provides data processing methods for Contoso SIS data (the student information system for the fictional Contoso school district).  \"\"\"\n",
					"    def __init__(self, source_folder, pseudonymize = True):\n",
					"        self.source_folder = source_folder\n",
					"        self.pseudonymize = pseudonymize\n",
					"        self.stage1np = f\"{oea.stage1np}/{source_folder}\"\n",
					"        self.stage2np = f\"{oea.stage2np}/{source_folder}\"\n",
					"        self.stage2p = f\"{oea.stage2p}/{source_folder}\"\n",
					"        self.stage3np = f\"{oea.stage3np}/{source_folder}\"\n",
					"        self.stage3p = f\"{oea.stage3p}/{source_folder}\"\n",
					"        self.module_path = f\"{oea.framework_path}/modules/{source_folder}\"\n",
					"        self.schemas = {}\n",
					"\n",
					"    def _process_entity_from_stage1(self, path, entity_name, format='csv', write_mode='overwrite', header='true'):\n",
					"        spark_schema = oea.to_spark_schema(self.schemas[entity_name])\n",
					"        df = spark.read.format(format).load(f\"{self.stage1np}/{path}/{entity_name}\", header=header, schema=spark_schema)\n",
					"\n",
					"        if self.pseudonymize:\n",
					"            df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[entity_name])\n",
					"            df_pseudo.write.format('delta').mode(write_mode).save(f\"{self.stage2p}/{entity_name}\")\n",
					"            if len(df_lookup.columns) > 0:\n",
					"                df_lookup.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}_lookup\")\n",
					"        else:\n",
					"            df = oea.fix_column_names(df)   \n",
					"            df.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}\")\n",
					"\n",
					"    def delete_stage1(self):\n",
					"        oea.rm_if_exists(self.stage1np)\n",
					"\n",
					"    def delete_stage2(self):\n",
					"        oea.rm_if_exists(self.stage2np)\n",
					"        oea.rm_if_exists(self.stage2p)\n",
					"\n",
					"    def delete_stage3(self):\n",
					"        oea.rm_if_exists(self.stage3np)\n",
					"        oea.rm_if_exists(self.stage3p)                \n",
					"\n",
					"    def delete_all_stages(self):\n",
					"        self.delete_stage1()\n",
					"        self.delete_stage2()\n",
					"        self.delete_stage3()\n",
					"\n",
					"    def create_stage2_lake_db(self, format='DELTA'):\n",
					"        oea.create_lake_db(self.stage2p, format)\n",
					"        oea.create_lake_db(self.stage2np, format)\n",
					"\n",
					"    def create_stage3_lake_db(self, format='DELTA'):\n",
					"        oea.create_lake_db(self.stage3p, format)\n",
					"        oea.create_lake_db(self.stage3np, format)\n",
					"\n",
					"    def copy_test_data_to_stage1(self):\n",
					"        mssparkutils.fs.cp(self.module_path + '/test_data', self.stage1np, True)   \n",
					"\n",
					"class DataLakeWriter:\n",
					"    def __init__(self, root_destination):\n",
					"        self.root_destination = root_destination\n",
					"\n",
					"    def write(self, path_and_filename, data_str, format='csv'):\n",
					"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
					"\n",
					"oea = OEA()"
				]
			}
		]
	}
}