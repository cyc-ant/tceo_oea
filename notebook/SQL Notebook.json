{
	"name": "SQL Notebook",
	"properties": {
		"folder": {
			"name": "Stage3"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p1sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "41a1d9e6-e207-47dd-a14d-72aa94e13435"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "sql"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/09921ef6-5d34-4a25-a233-84ffa9485571/resourceGroups/rg-oea-tceooeadev01/providers/Microsoft.Synapse/workspaces/syn-oea-tceooeadev01/bigDataPools/spark3p1sm",
				"name": "spark3p1sm",
				"type": "Spark",
				"endpoint": "https://syn-oea-tceooeadev01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p1sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"%run OEA_py"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"# example df\r\n",
					"data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\r\n",
					"        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\r\n",
					"        (\"Jen\",\"\",None)]\r\n",
					"\r\n",
					"columns = [\"name\",\"gender\",\"salary\"]\r\n",
					"df = spark.createDataFrame(data = data, schema = columns)\r\n",
					"\r\n",
					"df.createOrReplaceTempView('vw_df')\r\n",
					"\r\n",
					"#df.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"# source locations\r\n",
					"sourcep = oea.stage2p\r\n",
					"sourcenp = oea.stage2np\r\n",
					"\r\n",
					"test_source = sourcep + '/student/DMStudent_pseudo'\r\n",
					"\r\n",
					"# create source dataframe from stage 2\r\n",
					"test_df = spark.read.load(f\"{test_source}\", format='delta')\r\n",
					"\r\n",
					"# create a sql view of the dataframe\r\n",
					"test_df.createOrReplaceTempView('test_dmstudent_df')"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Testing Transform_Functions functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"## Put test function here\r\n",
					"\r\n",
					"# def DMStudent_pseudo(df):\r\n",
					"\r\n",
					"#     sourcep = oea.stage2p\r\n",
					"\r\n",
					"#     df.createOrReplaceTempView('vw_df_dmstudent_pseudo')\r\n",
					"\r\n",
					"#     final_df = spark.sql(\"select * from vw_df_dmstudent_pseudo where length(bksourcestudentid) = 36\")\r\n",
					"\r\n",
					"#     return final_df\r\n",
					"\r\n",
					"def DMPersonDemographic_pseudo(df):\r\n",
					"\r\n",
					"    sourcep = oea.stage2p\r\n",
					"\r\n",
					"    df.createOrReplaceTempView('vw_df_dmpersondemographic_pseudo')\r\n",
					"\r\n",
					"    final_df = spark.sql(\"select * from vw_df_dmpersondemographic_pseudo where PersonType = 'Student' or PersonType = 'Staff'\")\r\n",
					"\r\n",
					"    return final_df"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"def DMFactPATResults_pseudo(df):\r\n",
					"\r\n",
					"    sourcep = oea.stage2p\r\n",
					"\r\n",
					"    df.createOrReplaceTempView('vw_df_dmfactpatresults_pseudo')\r\n",
					"\r\n",
					"    datedetails_df = spark.read.load(sourcep + \"/ACER/DMPATStudentTestGrouping_pseudo\", format='delta')\r\n",
					"\r\n",
					"    datedetails_df.createOrReplaceTempView('vw_df_dmpatstudenttestgrouping')\r\n",
					"\r\n",
					"    final_df = spark.sql(\"select a.*, concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) as combineddatetime from vw_df_dmfactpatresults_pseudo a left join vw_df_dmpatstudenttestgrouping b on a.skpatstudenttestgrouping = b.skpatstudenttestgrouping inner join (select skyear, skacerstudent, skpattest, skcampus, scalescore, datecomplete, timecomplete, combineddatetime, max(skpatresults) as maxskpatresults from (select a.skpatresults, a.skyear, a.skacerstudent, a.skpattest, a.skcampus,  a.ScaleScore,  b.datecomplete, b.timecomplete, concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) as combineddatetime from vw_df_dmfactpatresults_pseudo a left join vw_df_dmpatstudenttestgrouping b on a.skpatstudenttestgrouping = b.skpatstudenttestgrouping inner join (select skyear, skacerstudent, skpattest, skcampus, combineddatetime, max(scalescore) as maxscalescore from (select a.skyear, a.skacerstudent, a.skpattest, a.skcampus,  a.ScaleScore,  b.datecomplete, b.timecomplete, concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) as combineddatetime from vw_df_dmfactpatresults_pseudo a left join vw_df_dmpatstudenttestgrouping b on a.skpatstudenttestgrouping = b.skpatstudenttestgrouping inner join (select skyear, skacerstudent, skpattest, skcampus, max(concat(datecomplete, ' ', right(left(timecomplete, 10), 8))) as latestdate from (select a.*, orig.skpatstudenttestgrouping, orig.skacerstanine, ScaleScore, skpatresults, b.datecomplete, b.timecomplete, concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) as combineddatetime from (select skyear, skacerstudent, skpattest, skcampus, count(*) as counttotal from vw_df_dmfactpatresults_pseudo where skpattest not in (43, 54, 65, 66, 76, 79, 92, 93, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112) group by skyear, skacerstudent, skpattest, skcampus) a left join vw_df_dmfactpatresults_pseudo orig on a.skyear = orig.skyear and a.skacerstudent = orig.skacerstudent and a.skpattest = orig.skpattest and a.skcampus = orig.skcampus left join vw_df_dmpatstudenttestgrouping b on orig.skpatstudenttestgrouping = b.skpatstudenttestgrouping) total group by skyear, skacerstudent, skpattest, skcampus) inner1 on a.skyear = inner1.skyear and a.skacerstudent = inner1.SKACERStudent and a.SKPATTest = inner1.SKPATTest and a.SKCampus = inner1.SKCampus and concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) = inner1.latestdate ) inner2 group by skyear, skacerstudent, skpattest, skcampus, combineddatetime ) inner3 on a.skyear = inner3.skyear and a.skacerstudent = inner3.SKACERStudent and a.SKPATTest = inner3.SKPATTest and a.SKCampus = inner3.SKCampus and concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) = inner3.combineddatetime ) inner4 group by skyear, skacerstudent, skpattest, skcampus, scalescore, datecomplete, timecomplete, combineddatetime ) inner5 on a.skyear = inner5.skyear and a.skacerstudent = inner5.SKACERStudent and a.SKPATTest = inner5.SKPATTest and a.SKCampus = inner5.SKCampus and concat(b.datecomplete, ' ', right(left(b.timecomplete, 10), 8)) = inner5.combineddatetime and a.SKPATResults = inner5.maxskpatresults\")\r\n",
					"\r\n",
					"    return final_df\r\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"%run Transform_Functions_py"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"## Test \r\n",
					"\r\n",
					"sourcep = oea.stage2p\r\n",
					"test_df = spark.read.load(sourcep + \"/ACER/DMFactPATResults_pseudo\", format='delta')\r\n",
					"\r\n",
					"result_test = DMFactPATResults_pseudo(test_df)\r\n",
					"# result_test.show()\r\n",
					"\r\n",
					"result_test.createOrReplaceTempView('vw_df_test_checkresult4')"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"## Test \r\n",
					"\r\n",
					"# sourcep = oea.stage2p\r\n",
					"# test_df = spark.read.load(sourcep + \"/student/DMStudent_pseudo\", format='delta')\r\n",
					"\r\n",
					"# result_test = DMStudent_pseudo(test_df)\r\n",
					"# # result_test.show()\r\n",
					"\r\n",
					"# result_test.createOrReplaceTempView('vw_df_test_checkresult2')\r\n",
					"\r\n",
					"sourcep = oea.stage2p\r\n",
					"test_df = spark.read.load(sourcep + \"/student/DMPersonDemographic_pseudo\", format='delta')\r\n",
					"\r\n",
					"result_test = DMPersonDemographic_pseudo(test_df)\r\n",
					"# result_test.show()\r\n",
					"\r\n",
					"result_test.createOrReplaceTempView('vw_df_test_checkresult3')"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"-- check sql results\r\n",
					"\r\n",
					"-- select count(*) from vw_df_test_checkresult2\r\n",
					"-- where length(bksourcestudentid) = 36\r\n",
					"\r\n",
					"select persontype, count(*) from vw_df_test_checkresult3 group by persontype"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"# setup the temp tables we need for the classenrolments table\r\n",
					"sourcep = oea.stage2p\r\n",
					"\r\n",
					"# used for classenrolment view\r\n",
					"factsubjectenrolment_df = spark.read.load(sourcep + \"/enrolment/DMFactSubjectEnrolment_pseudo\", format='delta')\r\n",
					"dmactivity_df = spark.read.load(sourcep + \"/enrolment/DMActivity_pseudo\", format='delta')\r\n",
					"dmsubject_df = spark.read.load(sourcep + \"/enrolment/DMSubject_pseudo\", format='delta')\r\n",
					"dmpersondemographic_df = spark.read.load(sourcep + \"/student/DMPersonDemographic_pseudo\", format='delta')\r\n",
					"dmyearlevel_df = spark.read.load(sourcep + \"/ACER/DMYearLevel_pseudo\", format='delta')\r\n",
					"dmcampus_df = spark.read.load(sourcep + \"/ACER/DMCampus_pseudo\", format='delta')\r\n",
					"dmdate_df = spark.read.load(sourcep + \"/ACER/DMDate_pseudo\", format='delta')\r\n",
					"dmcalendaryear_df = spark.read.load(sourcep + \"/ACER/DMCalendarYear_pseudo\", format='delta')\r\n",
					"\r\n",
					"# used for active student view\r\n",
					"factpersonmilestone_df = spark.read.load(sourcep + \"/student/DMFactPersonMilestone_pseudo\", format='delta')\r\n",
					"dmrollgroup_df = spark.read.load(sourcep + \"/enrolment/DMRollGroup_pseudo\", format='delta')\r\n",
					"\r\n",
					"# create temp sql views\r\n",
					"factsubjectenrolment_df.createOrReplaceTempView('vw_factsubjectenrolment')\r\n",
					"dmactivity_df.createOrReplaceTempView('vw_dmactivity')\r\n",
					"dmsubject_df.createOrReplaceTempView('vw_dmsubject')\r\n",
					"dmpersondemographic_df.createOrReplaceTempView('vw_dmpersondemographic')\r\n",
					"dmyearlevel_df.createOrReplaceTempView('vw_dmyearlevel')\r\n",
					"dmcampus_df.createOrReplaceTempView('vw_dmcampus')\r\n",
					"dmdate_df.createOrReplaceTempView('vw_dmdate')\r\n",
					"dmcalendaryear_df.createOrReplaceTempView('vw_dmcalendaryear')\r\n",
					"factpersonmilestone_df.createOrReplaceTempView('vw_factpersonmilestone')\r\n",
					"dmrollgroup_df.createOrReplaceTempView('vw_dmrollgroup')"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"SELECT SchoolID AS SchoolAGEID,\r\n",
					"    LocationID AS LocationAGEID,\r\n",
					"    c.CampusSuburbAndName,\r\n",
					"    pd.FirstName,\r\n",
					"    pd.LastName,\r\n",
					"    yl.YearLevelSifDesc AS YearLevel,\r\n",
					"    rg.RollGroupDesc AS RollGroup,\r\n",
					"    pd.Ceider,\r\n",
					"    pd.SKPerson\r\n",
					"\r\n",
					"FROM vw_factpersonmilestone f\r\n",
					"JOIN vw_dmpersondemographic pd ON f.SKPerson = pd.SKPerson\r\n",
					"JOIN vw_dmcampus c ON f.SKCampus = c.SKCampus\r\n",
					"JOIN vw_dmyearlevel yl ON f.SKYearLevel = yl.SKYearLevel\r\n",
					"JOIN vw_dmrollgroup rg ON f.SKRollGroup = rg.SKRollGroup\r\n",
					"WHERE f.IsCurrentMilestone = 'True'\r\n",
					"    AND f.IsActiveMilestone = 'True'\r\n",
					"    AND pd.PersonType = 'Student'\r\n",
					"ORDER BY SchoolAGEID, yl.YearLevelSifDesc, pd.FirstName, pd.LastName"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"SELECT cy.Year AS CalendarYear,\r\n",
					"    C.SchoolId AS AGEID,\r\n",
					"    c.CampusSuburbAndName,\r\n",
					"    student.FirstName  AS  StudentFirstName,\r\n",
					"    student.LastName AS StudentLastName,\r\n",
					"    Student.Ceider,\r\n",
					"    yl.YearLevelSifDesc AS YearLevel,\r\n",
					"    s.SubjectShortName AS Subject,\r\n",
					"    a.ActivityName AS  ClassName,\r\n",
					"    a.ActivityCode AS ClassCode,\r\n",
					"    ClassStartDate.date AS ClassStartDate,\r\n",
					"    ClassEndDate.date AS ClassEndDate,\r\n",
					"    staff.FirstName AS StaffFirstName,\r\n",
					"    staff.LastName AS StaffLastName,\r\n",
					"    c.CompassSchoolURL\r\n",
					"\r\n",
					"    FROM vw_factsubjectenrolment f\r\n",
					"    JOIN vw_dmactivity a ON f.SKActivity = a.SKActivity\r\n",
					"    AND a.ActivityTypeDescription = 'Standard Class'\r\n",
					"    JOIN vw_dmsubject s ON f.SKSubject = s.SKSubject\r\n",
					"    JOIN vw_dmpersondemographic student ON f.SKStudent = student.SKPerson\r\n",
					"    JOIN vw_dmpersondemographic staff ON f.SKStaff = staff.SKPerson\r\n",
					"    JOIN vw_dmyearlevel yl ON f.SKYearLevel = yl.SKYearLevel\r\n",
					"    JOIN vw_dmcampus c ON f.SKCampus = c.SKCampus\r\n",
					"    JOIN vw_dmdate ClassStartDate ON f.SKEnrolmentStartDate = ClassStartDate.SKDate\r\n",
					"\r\n",
					"    JOIN vw_dmdate ClassEndDate ON f.SKEnrolmentEndDate = ClassEndDate.SKDate\r\n",
					"    JOIN vw_dmcalendaryear cy ON f.SKYear = cy.SKYear\r\n",
					"\r\n",
					"    AND cy.Year = 2022\r\n",
					"    --WHERE ClassEndDate.date > date_format('1/10/2022', 'd/mm/yyyy') --current_date() -- spark sql uses current_date() instead of GETDATE()\r\n",
					"    --AND s.SubjectShortName <> 'Unknown'\r\n",
					"    ORDER BY\r\n",
					"    AGEID, YearLevel, StudentFirstName, StudentLastName,Ceider"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"SELECT SchoolID AS SchoolAGEID,\r\n",
					"    LocationID AS LocationAGEID,\r\n",
					"    c.CampusSuburbAndName,\r\n",
					"    pd.FirstName,\r\n",
					"    pd.LastName,\r\n",
					"    yl.YearLevelSifDesc AS YearLevel,\r\n",
					"    rg.RollGroupDesc AS RollGroup,\r\n",
					"    pd.Ceider,\r\n",
					"    d.date AS DateLeft\r\n",
					"FROM\r\n",
					"    vw_factpersonmilestone f\r\n",
					"JOIN vw_dmdate d ON f.SKMilestoneStartDate = d.SKDate\r\n",
					"JOIN vw_dmpersondemographic pd ON f.SKPerson = pd.SKPerson\r\n",
					"JOIN vw_dmcampus c ON f.SKCampus = c.SKCampus\r\n",
					"JOIN vw_dmyearlevel yl ON f.SKYearLevel = yl.SKYearLevel\r\n",
					"JOIN vw_dmrollgroup rg ON f.SKRollGroup = rg.SKRollGroup\r\n",
					"\r\n",
					"WHERE \r\n",
					"    f.IsCurrentMilestone = 'True'\r\n",
					"    AND f.IsActiveMilestone = 'False'\r\n",
					"    AND pd.PersonType = 'Student'\r\n",
					"    AND Date < current_date()\r\n",
					"ORDER BY SchoolAGEID,yl.YearLevelSifDesc, pd.FirstName,pd.LastName"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"from pyspark.sql.functions import max\r\n",
					"\r\n",
					"\r\n",
					"sourcep = oea.stage2p\r\n",
					"df = spark.read.load(sourcep + \"/ACER/DMFactPATResults_pseudo\", format='delta')\r\n",
					"\r\n",
					"#aggdf = df.groupBy(\"SKCampus\", \"SKYearLevel\", \"SKDate\", \"SKPATTest\").agg(max(\"NormMeanScaledScore\").alias(\"newNormMean\"))\r\n",
					"aggdf = df.groupBy(\"SKPATTest\").agg(max(\"NormMeanScaledScore\").alias(\"NormMeanScaledScore_cleaned\"))\r\n",
					"aggdf = aggdf.withColumnRenamed(\"SKPATTest\", \"xSKPATTest\")\r\n",
					"\r\n",
					"final_df = aggdf.join(df, df.SKPATTest == aggdf.xSKPATTest, \"inner\")\r\n",
					"final_df = final_df.drop(\"xSKPATTest\")\r\n",
					"\r\n",
					"final_df.show(truncate=False)\r\n",
					"\r\n",
					"#column=[\"newNormMean\"]\r\n",
					"#aggdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in aggdf.columns]).show()\r\n",
					"\r\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"\r\n",
					"from pyspark.sql.functions import max\r\n",
					"from pyspark.sql.functions import col\r\n",
					"\r\n",
					"\r\n",
					"sourcep = oea.stage2p\r\n",
					"df = spark.read.load(sourcep + \"/ACER/DMFactPATResults_pseudo\", format='delta')\r\n",
					"\r\n",
					"#aggdf = df.groupBy(\"SKCampus\", \"SKYearLevel\", \"SKDate\", \"SKPATTest\").agg(max(\"NormMeanScaledScore\").alias(\"newNormMean\"))\r\n",
					"#aggdf = df.groupBy(\"SKYear\", \"SKCampus\", \"SKPATTest\", \"MatchedYearLevelTestLevel\").count().orderBy(col(\"count\").desc())\r\n",
					"\r\n",
					"aggdf = df.groupBy(\"SKYear\", \"SKCampus\", \"SKPATTest\", \"MatchedYearLevelTestLevel\").agg(max(\"NormMeanScaledScore\").alias(\"NormMeanScaledScore_cleaned\"))\r\n",
					"\r\n",
					"aggdf.show(truncate=False)"
				],
				"execution_count": 15
			}
		]
	}
}